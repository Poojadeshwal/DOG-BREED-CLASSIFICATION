{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71e7a7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all required libraries for Dog's Breed Identification Project\n",
    "import cv2\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.applications.resnet_v2 import ResNet50V2, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aa53aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the csv file\n",
    "df_labels = pd.read_csv(\"D:\\\\Dog Breed Classification\\\\Data Set\\\\labels.csv\")\n",
    "#store training and testing images folder location\n",
    "train_file = \"D:\\\\Dog Breed Classification\\\\Data Set\\\\train\\\\\"\n",
    "test_file = \"D:\\\\Dog Breed Classification\\\\Data Set\\\\test\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42bb88bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     id                     breed\n",
      "0      000bec180eb18c7604dcecc8fe0dba07               boston_bull\n",
      "1      001513dfcb2ffafc82cccf4d8bbaba97                     dingo\n",
      "2      001cdf01b096e06d78e9e5112d419397                  pekinese\n",
      "3      00214f311d5d2247d5dfe4fe24b2303d                  bluetick\n",
      "4      0021f9ceb3235effd7fcde7f7538ed62          golden_retriever\n",
      "...                                 ...                       ...\n",
      "10217  ffd25009d635cfd16e793503ac5edef0                    borzoi\n",
      "10218  ffd3f636f7f379c51ba3648a9ff8254f            dandie_dinmont\n",
      "10219  ffe2ca6c940cddfee68fa3cc6c63213f                  airedale\n",
      "10220  ffe5f6d8e2bff356e9482a80a6e29aac        miniature_pinscher\n",
      "10221  fff43b07992508bc822f33d8ffd902ae  chesapeake_bay_retriever\n",
      "\n",
      "[10222 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40b198d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique Dog Breeds : 120\n"
     ]
    }
   ],
   "source": [
    "    print(\"Total number of unique Dog Breeds :\",len(df_labels.breed.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3d80e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify number\n",
    "num_breeds = 120\n",
    "im_size = 224\n",
    "batch_size = 64\n",
    "encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "002c55f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all 120 unique breeds record \n",
    "breed_dict = list(df_labels['breed'].value_counts().keys()) \n",
    "new_list = sorted(breed_dict,reverse=True)[:num_breeds+1:1]\n",
    "#change the dataset to have those 120 unique breed records\n",
    "df_labels = df_labels.query('breed in @new_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0816f117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        000bec180eb18c7604dcecc8fe0dba07.jpg\n",
      "1        001513dfcb2ffafc82cccf4d8bbaba97.jpg\n",
      "2        001cdf01b096e06d78e9e5112d419397.jpg\n",
      "3        00214f311d5d2247d5dfe4fe24b2303d.jpg\n",
      "4        0021f9ceb3235effd7fcde7f7538ed62.jpg\n",
      "                         ...                 \n",
      "10217    ffd25009d635cfd16e793503ac5edef0.jpg\n",
      "10218    ffd3f636f7f379c51ba3648a9ff8254f.jpg\n",
      "10219    ffe2ca6c940cddfee68fa3cc6c63213f.jpg\n",
      "10220    ffe5f6d8e2bff356e9482a80a6e29aac.jpg\n",
      "10221    fff43b07992508bc822f33d8ffd902ae.jpg\n",
      "Name: img_file, Length: 10222, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#create new column which will contain image name with the image extension\n",
    "df_labels['img_file'] = df_labels['id'].apply(lambda x: x + \".jpg\")\n",
    "print(df_labels['img_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01c274d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a numpy array of the shape\n",
    "#(number of dataset records, image size , image size, 3 for rgb channel ayer)\n",
    "#this will be input for model\n",
    "train_x = np.zeros((len(df_labels), im_size, im_size, 3), dtype='float32')\n",
    " \n",
    "#iterate over img_file column of our dataset\n",
    "for i, img_id in enumerate(df_labels['img_file']):\n",
    "  #read the image file and convert into numeric format\n",
    "  #resize all images to one dimension i.e. 224x224\n",
    "  #we will get array with the shape of\n",
    "  # (224,224,3) where 3 is the RGB channels layers\n",
    "  img = cv2.resize(cv2.imread(train_file+img_id,cv2.IMREAD_COLOR),((im_size,im_size)))\n",
    "  #scale array into the range of -1 to 1.\n",
    "  #preprocess the array and expand its dimension on the axis 0 \n",
    "  img_array = preprocess_input(np.expand_dims(np.array(img[...,::-1].astype(np.float32)).copy(), axis=0))\n",
    "  #update the train_x variable with new element\n",
    "  train_x[i] = img_array\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6832e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will be the target for the model.\n",
    "#convert breed names into numerical format\n",
    "train_y = encoder.fit_transform(df_labels[\"breed\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbb6c405",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset in the ratio of 80:20. \n",
    "#80% for training and 20% for testing purpose\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_x,train_y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "241bbb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image augmentation using ImageDataGenerator class\n",
    "train_datagen = ImageDataGenerator(rotation_range=45,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.25,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    " \n",
    "#generate images for training sets \n",
    "train_generator = train_datagen.flow(x_train, \n",
    "                                     y_train, \n",
    "                                     batch_size=batch_size)\n",
    " \n",
    "#same process for Testing sets also by declaring the instance\n",
    "test_datagen = ImageDataGenerator()\n",
    " \n",
    "test_generator = test_datagen.flow(x_test, \n",
    "                                     y_test, \n",
    "                                     batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6cd505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the model using ResNet50V2 with input shape of our image array\n",
    "#weights for our network will be from of imagenet dataset\n",
    "#we will not include the first Dense layer\n",
    "resnet = ResNet50V2(input_shape = [im_size,im_size,3], weights='imagenet', include_top=False)\n",
    "#freeze all trainable layers and train only top layers \n",
    "for layer in resnet.layers:\n",
    "    layer.trainable = False\n",
    " \n",
    "#add global average pooling layer and Batch Normalization layer\n",
    "x = resnet.output\n",
    "x = BatchNormalization()(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "#add fully connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecca9123",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add output layer having the shape equal to number of breeds\n",
    "predictions = Dense(num_breeds, activation='softmax')(x)\n",
    " \n",
    "#create model class with inputs and outputs\n",
    "model = Model(inputs=resnet.input, outputs=predictions)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "454c2157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "127/127 [==============================] - 767s 6s/step - loss: 2.7246 - accuracy: 0.3533 - val_loss: 1.0618 - val_accuracy: 0.6799\n",
      "Epoch 2/20\n",
      "127/127 [==============================] - 1334s 11s/step - loss: 1.6569 - accuracy: 0.5454 - val_loss: 0.9375 - val_accuracy: 0.7162\n",
      "Epoch 3/20\n",
      "127/127 [==============================] - 813s 6s/step - loss: 1.4814 - accuracy: 0.5881 - val_loss: 0.8802 - val_accuracy: 0.7440\n",
      "Epoch 4/20\n",
      "127/127 [==============================] - 820s 6s/step - loss: 1.4226 - accuracy: 0.6029 - val_loss: 0.8897 - val_accuracy: 0.7334\n",
      "Epoch 5/20\n",
      "127/127 [==============================] - 786s 6s/step - loss: 1.3657 - accuracy: 0.6227 - val_loss: 0.8896 - val_accuracy: 0.7424\n",
      "Epoch 6/20\n",
      "127/127 [==============================] - 809s 6s/step - loss: 1.2911 - accuracy: 0.6345 - val_loss: 0.9150 - val_accuracy: 0.7369\n",
      "Epoch 7/20\n",
      "127/127 [==============================] - 822s 6s/step - loss: 1.2834 - accuracy: 0.6391 - val_loss: 0.9021 - val_accuracy: 0.7434\n",
      "Epoch 8/20\n",
      "127/127 [==============================] - 832s 7s/step - loss: 1.2597 - accuracy: 0.6470 - val_loss: 0.9358 - val_accuracy: 0.7379\n",
      "Epoch 9/20\n",
      "127/127 [==============================] - 840s 7s/step - loss: 1.2172 - accuracy: 0.6517 - val_loss: 0.9343 - val_accuracy: 0.7455\n",
      "Epoch 10/20\n",
      "127/127 [==============================] - 862s 7s/step - loss: 1.1742 - accuracy: 0.6695 - val_loss: 0.9266 - val_accuracy: 0.7389\n",
      "Epoch 11/20\n",
      "127/127 [==============================] - 752s 6s/step - loss: 1.1781 - accuracy: 0.6663 - val_loss: 0.9345 - val_accuracy: 0.7379\n",
      "Epoch 12/20\n",
      "127/127 [==============================] - 772s 6s/step - loss: 1.1560 - accuracy: 0.6700 - val_loss: 0.9541 - val_accuracy: 0.7404\n",
      "Epoch 13/20\n",
      "127/127 [==============================] - 782s 6s/step - loss: 1.1627 - accuracy: 0.6777 - val_loss: 0.9439 - val_accuracy: 0.7445\n",
      "Epoch 14/20\n",
      "127/127 [==============================] - 725s 6s/step - loss: 1.1375 - accuracy: 0.6800 - val_loss: 0.9616 - val_accuracy: 0.7419\n",
      "Epoch 15/20\n",
      "127/127 [==============================] - 787s 6s/step - loss: 1.1355 - accuracy: 0.6810 - val_loss: 0.9556 - val_accuracy: 0.7495\n",
      "Epoch 16/20\n",
      "127/127 [==============================] - 710s 6s/step - loss: 1.1324 - accuracy: 0.6816 - val_loss: 0.9832 - val_accuracy: 0.7455\n",
      "Epoch 17/20\n",
      "127/127 [==============================] - 762s 6s/step - loss: 1.1084 - accuracy: 0.6894 - val_loss: 0.9902 - val_accuracy: 0.7399\n",
      "Epoch 18/20\n",
      "127/127 [==============================] - 796s 6s/step - loss: 1.1274 - accuracy: 0.6805 - val_loss: 0.9772 - val_accuracy: 0.7445\n",
      "Epoch 19/20\n",
      "127/127 [==============================] - 800s 6s/step - loss: 1.0984 - accuracy: 0.6899 - val_loss: 1.0076 - val_accuracy: 0.7419\n",
      "Epoch 20/20\n",
      "127/127 [==============================] - 803s 6s/step - loss: 1.0984 - accuracy: 0.6980 - val_loss: 1.0075 - val_accuracy: 0.7434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    }
   ],
   "source": [
    "#epochs for model training and learning rate for optimizer\n",
    "epochs = 20\n",
    "learning_rate = 1e-3\n",
    " \n",
    "#using RMSprop optimizer to compile or build the model\n",
    "optimizer = RMSprop(learning_rate=learning_rate,rho=0.9)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[\"accuracy\"])\n",
    " \n",
    "#fit the training generator data and train the model\n",
    "hist = model.fit(train_generator,\n",
    "                 steps_per_epoch= x_train.shape[0] // batch_size,\n",
    "                 epochs= epochs,\n",
    "                 validation_data= test_generator,\n",
    "                 validation_steps= x_test.shape[0] // batch_size)\n",
    " \n",
    "#Save the model for prediction\n",
    "model.save(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2f6e699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "Predicted Breed for this Dog is : afghan_hound\n"
     ]
    }
   ],
   "source": [
    "#load the model\n",
    "model = load_model(\"model\")\n",
    " \n",
    "#get the image of the dog for prediction\n",
    "pred_img_path = \"C:\\\\Users\\\\asus\\\\Dropbox\\\\My PC (LAPTOP-KVI3FLV1)\\\\Downloads\\\\Pooh.jpg\"\n",
    "#read the image file and convert into numeric format\n",
    "#resize all images to one dimension i.e. 224x224\n",
    "pred_img_array = cv2.resize(cv2.imread(pred_img_path,cv2.IMREAD_COLOR),((im_size,im_size)))\n",
    "#scale array into the range of -1 to 1.\n",
    "#expand the dimension on the axis 0 and normalize the array values\n",
    "pred_img_array = preprocess_input(np.expand_dims(np.array(pred_img_array[...,::-1].astype(np.float32)).copy(), axis=0))\n",
    " \n",
    "#feed the model with the image array for prediction\n",
    "pred_val = model.predict(np.array(pred_img_array,dtype=\"float32\"))\n",
    " \n",
    "#display the image of dog\n",
    "cv2.imshow(\"Dog Image\",cv2.resize(cv2.imread(pred_img_path,cv2.IMREAD_COLOR),((im_size,im_size)))) \n",
    " \n",
    "#display the predicted breed of dog\n",
    "pred_breed = sorted(new_list)[np.argmax(pred_val)]\n",
    "print(\"Predicted Breed for this Dog is :\",pred_breed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce180292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64099a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
